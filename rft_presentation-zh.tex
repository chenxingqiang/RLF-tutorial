% beamer幻灯片类，采用16:9宽高比
\documentclass[aspectratio=169]{beamer}

% 加载所有软件包
\input{packages}

% 添加中文支持
\usepackage{xeCJK}
\usepackage{ctex}
\setCJKmainfont{STSong}
\setCJKsansfont{STHeiti}
\setCJKmonofont{STFangsong}

% 确保所有字体都能正确显示中文
\newfontfamily\zhfont{STSong}
\newfontfamily\zhsffont{STHeiti}
\newfontfamily\zhttfont{STFangsong}

% 设置beamer字体
\usefonttheme{professionalfonts}
\setbeamerfont{title}{family=\zhsffont}
\setbeamerfont{frametitle}{family=\zhsffont}
\setbeamerfont{framesubtitle}{family=\zhsffont}

% 使用修改后的OVGU主题，带有之江实验室标志
\usepackage{beamer_ovgu_169}

% 全局设置以防止内容溢出
\usepackage{pgfplots}
\usetikzlibrary{patterns}

% 优化幻灯片布局以防止溢出
\setbeamersize{text margin left=0.5cm, text margin right=0.5cm}
\setbeamertemplate{frametitle}[default][center]

% 设置全局框架选项以自动处理溢出
\makeatletter
\define@key{beamerframe}{shrink}[100]{%
  \def\beamer@shrinkpercentage{#1}}
\makeatother
\BeforeBeginEnvironment{frame}{% 
  \setkeys{beamerframe}{shrink=45}% 
}

% 全局设置以处理中文内容溢出问题
\addtobeamertemplate{frametitle}{}{\vspace{-0.2cm}}
\addtobeamertemplate{block begin}{}{\vspace{-0.2cm}}
\addtobeamertemplate{itemize item}{}{\vspace{-0.1cm}}

% 对长框架的额外调整
\newenvironment{compactframe}{
  \begin{frame}[shrink=60]
  \setbeamerfont{itemize/enumerate body}{size=\scriptsize}
  \setbeamerfont{itemize/enumerate subbody}{size=\scriptsize}
}{\end{frame}}

% Adjust spacing throughout the presentation
\addtobeamertemplate{frametitle}{}{\vspace{-0.3cm}}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\partopsep}{0pt}

% Adjust beamer spacing settings to be more compact
\AtBeginDocument{
  \setbeamerfont{itemize/enumerate body}{size=\small}
  \setbeamerfont{itemize/enumerate subbody}{size=\small}
  \setbeamerfont{itemize/enumerate subsubbody}{size=\small}
}

% 禁用参考文献功能，避免bbl文件缺失警告
\providecommand{\bibfont}{}
\renewcommand{\bibfont}{\normalfont\footnotesize}
\AtEndDocument{\def\bibfont{}}
\let\printbibliography\relax

\title[强化微调指南 2025]{强化微调完全指南}
\author{陈星强}
\institute[亿铸智能]{
	亿铸智能
    \\
	中国杭州
}
\date[2025-04-25]{2025年4月25日}

\begin{document}

\begin{frame}
	\maketitle
\end{frame}

\begin{frame}[label=inhalt]{内容概述}
	\tableofcontents
\end{frame}

\section{使用DeepSeek-R1和强化学习重塑AI}

\begin{frame}
	\frametitle{强化学习简介}
	\begin{block}{自我提升范式}
		\begin{itemize}
			\item 强化学习引入了一种基于反馈驱动的AI训练机制
			\item 相比依赖标记样本，强化学习代理通过以下方式学习：
			\begin{itemize}
				\item \textbf{探索}：模型尝试多种策略或行动
				\item \textbf{奖励}：每个行动产生指导未来选择的奖励信号
			\end{itemize}
			\item 更接近人类通过尝试、错误和反馈自然学习的方式
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{强化学习简介 }
	\begin{block}{自我提升范式}
		\begin{itemize}
			\item 为持续学习和更深层的推理能力打开了大门
			\item AI不再是静态的实践—模型可以在部署后进化和适应
		\end{itemize}
	\end{block}
	\begin{block}{从静态到动态学习}
		\begin{itemize}
			\item 传统方法依赖大量标记数据集进行记忆
			\item 强化学习将焦点转移到学习策略和推理模式
			\item “环境”可以是任何提供反馈信号指导改进的场景
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{DeepSeek-R1的重要性}
	\begin{block}{DeepSeek-R1：一个为推理而设计的模型}
		\begin{itemize}
			\item \textbf{自适应奖励结构}：多重奖励函数聚焦于准确性、效率性和创造性
			\item \textbf{迭代精化}：基于奖励的反馈循环强调实践中最有效的方法
			\item \textbf{突破性能障碍}：持续学习使其能够超越传统大语言模型
			\item \textbf{开源优势}：与OpenAI闭源的O1不同，DeepSeek-R1共享模型权重和训练方法
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{\textbf{DeepSeek-R1}: 技术实现}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{主动探索机制}
				\begin{itemize}\setlength{\itemsep}{0em}
					\item 使用\textbf{主动探索}而非被动学习
					\item 采用\textbf{基于奖励的反馈循环}
					\item 平衡探索与利用
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{推理模型民主化}
				\begin{itemize}\setlength{\itemsep}{0em}
					\item \textbf{开源设计}便于自定义
					\item \textbf{发布模型权重}保证透明度
					\item \textbf{社区驱动}的创新
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[shrink=20]
	\frametitle{DeepSeek-R1 与传统模型对比}
	\vspace{-0.2cm}
	\begin{block}{关键差异：}
		\setlength{\itemsep}{0.5em}
		\begin{itemize}
			\item \textbf{传统模型：} 静态训练，固定参数
			\item \textbf{DeepSeek-R1：} 动态学习方法
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{DeepSeek-R1 与传统模型对比 (继续)}
	\begin{block}{DeepSeek-R1 优势：}
		\begin{itemize}
			\item 动态学习，响应需求变化
			\item 通过奖励学习，减少对标记数据的依赖
			\item 通过持续学习降低停滞风险
		\end{itemize}
	\end{block}
	\begin{block}{关键见解：}
		\begin{itemize}
			\item “一次性训练完成”模式正在成为过去式
		\end{itemize}
	\end{block}
\end{frame}

\section{强化微调与监督微调对比}

\begin{frame}
	\frametitle{什么是强化微调(RFT)？}
	\begin{block}{将微调与强化学习相结合}
		\begin{itemize}
			\item RFT结合了预训练大语言模型与基于反馈的强化学习的优势
			\item 核心过程：
			\begin{enumerate}
				\item 从具有通用知识的预训练模型开始
				\item 为目标指标定义奖励函数
				\item 使用强化学习技术进行迭代微调
			\end{enumerate}
			\item 使用最少的数据实现开源大语言模型的自定义
			\item 将通用模型转变为针对特定任务的强大推理模型
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{RFT与SSFT对比}
	\begin{table}
		\footnotesize
		\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|}
			\hline
			\textbf{因素} & \textbf{RFT} & \textbf{SFT} \\
			\hline
			数据需求 & 最少的标记数据 & 需要1,000+行数据 \\
			\hline
			适应性 & 持续改进 & 受标记数据限制 \\
			\hline
			探索能力 & 主动尝试新策略 & 依赖固定示例 \\
			\hline
			性能 & 持续进步 & 达到平台期 \\
			\hline
			错误处理 & 从错误中学习 & 重复数据中的错误 \\
			\hline
			训练复杂度 & 较高（奖励函数） & 较低（仅需示例） \\
			\hline
		\end{tabular}
		\caption{RFT和SFT方法的比较}
	\end{table}
\end{frame}

\begin{frame}
	\frametitle{RFT的优势场景}
	\begin{block}{RFT在数据稀缺时表现出色}
		\begin{itemize}
			\item 无需标记数据，依靠客观正确性
			\item 在小型数据集（几十个示例）上优于SFT
			\item 通过学习稳健策略抵抗过拟合
			\item 最佳应用场景：
			\begin{itemize}
				\item 代码转译（如Java到Python）
				\item 游戏策略（国际象棋，Wordle）
				\item 医疗诊断（在决策点从反馈中学习）
			\end{itemize}
			\end{itemize}
		\end{block}
\end{frame}

\begin{frame}
	\frametitle{定量性能比较}
	\begin{block}{按数据集大小比较RFT与SFT}
		\begin{itemize}
			\item \textbf{10个示例}：RFT使基础模型提升18\%，SFT仅显示最小增益
			\item \textbf{50个示例}：RFT相比基准线显示42\%的提升
			\item \textbf{100个示例}：RFT提升跃升至60\%，比SFT好3倍
			\item 数据集越小，RFT相比SFT的优势越明显
		\end{itemize}
	\end{block}
	\begin{figure}[ht]
		\centering
		\begin{tikzpicture}[scale=0.5, transform shape]
			\begin{axis}[
				xlabel={数据集大小},
				ylabel={性能提升 (\%)},
				legend style={at={(0.5,-0.15)}, anchor=north},
				ymin=0,
				ymax=70,
				grid=both,
				minor tick num=1,
				width=8cm,
				height=5cm,
				legend columns=2,
				scaled ticks=false,
				font=\footnotesize,
				xtick={10, 50, 100}]
				\addplot[color=blue, mark=*,line width=1.2pt] coordinates {
					(10,18) (50,42) (100,60)
				};
				\addplot[color=red, mark=square*,line width=1.2pt] coordinates {
					(10,5) (50,15) (100,20)
				};
				\legend{RFT, SFT};
			\end{axis}
		\end{tikzpicture}
		\caption{性能提升与数据集大小的关系}
	\end{figure}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{何时使用RFT与SFT}
	\begin{block}{决策因素:}
		\begin{itemize}
			\item \textbf{数据可用性}: 数据有限时选择RFT；有大量标记数据时选择SFT
			\item \textbf{任务复杂性}: 对于有明确成功标准的任务，选择RFT
			\item \textbf{性能目标}: 需要持续改进时选择RFT；需要稳定结果时选择SFT
			\item \textbf{可验证性}: 当结果可以客观衡量时，RFT表现更佳
			\item \textbf{资源限制}: SFT初期实施更简单，但需要更多数据
		\end{itemize}
	\end{block}
	\begin{block}{实际应用:}
		\begin{itemize}
			\item \textbf{编程助手}: RFT训练模型编写可编译并通过测试的代码
			\item \textbf{数据分析}: RFT改进生成准确结果的查询能力
			\item \textbf{推理任务}: RFT增强逐步解决问题的能力
		\end{itemize}
	\end{block}
\end{frame}

\section{使用Turbo LoRA加速推理模型}

\begin{frame}[shrink=10]
	\frametitle{推理模型的挑战}
	\begin{block}{处理量问题:}
		\begin{itemize}
			\item 推理模型推动了AI的解决问题能力
			\item 但高级推理需要付出代价：处理速度缓慢
			\item 推理模型通过生成大量标记来“思考”
			\item 多重中间计算增加了处理时间
			\item 生产部署需要解决这些挑战
		\end{itemize}
	\end{block}
	\begin{block}{实际性能瓶颈}
		\begin{itemize}
			\item 推理模型比类似的非推理模型慢2-3倍
			\item 更高的延迟显著影响用户体验和成本效率
			\item 传统加速方法常常会损害推理质量
			\item 需要特殊解决方案，在提高速度的同时保持推理能力
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{LoRA和Turbo LoRA}
	\begin{block}{LoRA: 低秩适应}
		\begin{itemize}
			\item 使用小型可训练参数集微调大型模型
			\item 保留原始权重，维持预训练知识
			\item 显著减少微调所需的内存要求
			\item 支持模型高效适应专业任务
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[shrink=15]
	\frametitle{Turbo LoRA: 推理速度提升2-4倍}
	\begin{block}{主要特点}
		\begin{itemize}
			\item 使用推测解码以及专有优化
			\item 并行预测多个标记，然后验证它们
			\item 在更快生成文本的同时保持输出质量
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[shrink=20]
	\frametitle{Turbo LoRA如何工作}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{技术实现}
				\begin{enumerate}\setlength{\itemsep}{0.5em}
					\item 小型、快速的“推测器”模型并行预测多个标记
					\item 主模型验证预测的标记
					\item 正确的标记立即被使用
					\item 只有不正确的标记需要重新计算
				\end{enumerate}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{实际效益}
				\begin{itemize}\setlength{\itemsep}{0.5em}
					\item 最终输出质量零差异
					\item 应用于DeepSeek-R1-distill-qwen-32b模型
					\item 实现了2-3倍的处理量提升
					\item 可应用于任何推理模型
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
	\vspace{0.2cm}
	\begin{center}
		\resizebox{0.85\textwidth}{!}{
		\begin{tikzpicture}
			\draw[thick,->] (0,0) -- (9,0) node[right] {时间};
			% Standard generation
			\draw[thick] (0,1) -- (9,1);
			\foreach \x in {1,2,3,4,5,6,7,8}
				\draw[fill=gray!30] (\x,0.7) rectangle (\x+0.5,1.3) node[pos=.5] {$t_\x$};
			% Turbo generation
			\draw[thick] (0,2.5) -- (9,2.5);
			\draw[fill=blue!20] (1,2.2) rectangle (4,2.8) node[pos=.5] {$t_{1-4}$};
			\draw[fill=blue!20] (4.5,2.2) rectangle (7.5,2.8) node[pos=.5] {$t_{5-8}$};
			% Labels
			\node at (-1,1) {标准:};
			\node at (-1,2.5) {Turbo:};
		\end{tikzpicture}
		}
	\end{center}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{Turbo对大型推理模型的益处}
	\begin{block}{实现实时AI可行性}
		\begin{itemize}
			\item 2-3倍的速度提升使推理模型适用于:
			\begin{itemize}
				\item AI驱动的客户支持
			\end{itemize}
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{Turbo对大型推理模型的益处（续）}
	\begin{block}{实现实时AI可行性（续）}
		\begin{itemize}
			\item 2-3倍的速度提升使推理模型适用于:
			\begin{itemize}
				\item 开发者的AI副驾驶
				\item 医疗保健AI助手
			\end{itemize}
			\item 降低GPU成本：相同工作量需要更少的GPU
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{大型推理模型Turbo的优势 }
	\begin{block}{实际影响}
		\begin{itemize}
			\item 使几乎任何组织都能负担推理模型的运行
			\item 典型推理设置：所需GPU容量减少2-3倍
			\item 成本节约随部署规模扩大而增加
		\end{itemize}
	\end{block}
	\begin{block}{实施方法}
		\begin{itemize}
			\item 可使用更小、更高效的模型而不牺牲能力
			\item 详细实施教程: \texttt{https://predibase.com/blog/turbo-lora}
		\end{itemize}
	\end{block}
\end{frame}

\section{教程: 使用RFT编写CUDA核心}

\begin{frame}[shrink=10]
	\frametitle{GPU代码生成为何困难}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{GPU编程的挑战:}
				\begin{itemize}\setlength{\itemsep}{0em}
					\item \textbf{并行架构}: 多核心
					\item \textbf{内存层次}: 多个层级
					\item \textbf{线程同步}: 复杂
					\item 小错误 → 大后果
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{传统方法的失败:}
				\begin{itemize}\setlength{\itemsep}{0em}
					\item 高质量CUDA示例稀少
					\item SFT需要成千个配对
					\item 需要处理多种边缘情况
					\item 细微错误导致失败
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\mode<all>{\vspace{-0.5cm}}

\begin{frame}[t]
	\frametitle{RFT为何适合代码生成}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{关键优势:}
				\begin{itemize}
					\item 无需大型数据集
					\item 代码可验证
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{实现:}
				\begin{itemize}
					\item 从13个示例开始
					\item 强化学习智能探索
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{设置CUDA任务: 最小数据集}
	\begin{block}{数据集组成}
		\begin{itemize}
			\item 每个示例包含:
			\begin{itemize}
				\item 一个PyTorch函数（如矩阵乘法或激活函数）
				\item 一组用于验证正确性的测试用例
			\end{itemize}
			\item 示例函数包括矩阵运算、激活函数和元素级运算
			\item 测试用例涵盖边缘情况、不同大小和边界条件
		\end{itemize}
	\end{block}
	\begin{block}{PyTorch到Triton示例}
	\begin{itemize}
		\item \textbf{PyTorch函数:}
		\begin{quote}
		\texttt{def add(x, y): return x + y}
		\end{quote}
	\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Setting Up the CUDA Task: Triton Implementation}
	\begin{block}{Target Triton Kernel}
	\begin{quote}
	\texttt{@triton.jit}\\  
	\texttt{def add\_kernel(x\_ptr, y\_ptr, output\_ptr, n\_elements):}\\  
	\texttt{\phantom{xxxx}pid = tl.program\_id(0)}\\  
	\texttt{\phantom{xxxx}block\_size = 128}\\  
	\texttt{\phantom{xxxx}offsets = pid * block\_size + tl.arange(0, block\_size)}
	\end{quote}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{设置CUDA任务: Triton实现（继续）}
	\begin{block}{目标Triton核心（继续）}
	\begin{quote}
	\texttt{\phantom{xxxx}mask = offsets < n\_elements}\\  
	\texttt{\phantom{xxxx}x = tl.load(x\_ptr + offsets, mask=mask)}\\  
	\texttt{\phantom{xxxx}y = tl.load(y\_ptr + offsets, mask=mask)}\\  
	\texttt{\phantom{xxxx}output = x + y}\\  
	\texttt{\phantom{xxxx}tl.store(output\_ptr + offsets, output, mask=mask)}
	\end{quote}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{为代码生成定义奖励}
	\begin{block}{多层次奖励结构 (1)}
		\begin{description}
			\item[\textbf{奖励1:}] \textbf{格式化 (0.1-0.3)}
		\end{description}
		\begin{itemize}
			\item 代码结构、导入和标签
			\item 为良好Triton语义给予部分分数
			\item 合理的变量名和代码组织
		\end{itemize}
	\end{block}
	\begin{block}{多层次奖励结构 (2)}
		\begin{description}
			\item[\textbf{奖励2:}] \textbf{编译 (0.3-0.6)}
		\end{description}
		\begin{itemize}
			\item 无错误执行的代码
			\item 无运行时异常或语法错误
			\item 正确导入所需依赖
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{为代码生成定义奖励（继续）}
	\begin{block}{多层次奖励结构 (3)}
		\begin{description}
			\item[\textbf{奖励3:}] \textbf{正确性 (0.6-1.0)}
		\end{description}
	\end{block}
	\begin{itemize}
		\item 输出与测试输入上的PyTorch函数匹配
		\item 反奖励黑客措施（检查硬编码输出）
		\item 正确处理边缘情况和不同输入形状
	\end{itemize}
	\vspace{0.5em}
	\begin{block}{该奖励结构的益处}
		\begin{itemize}
			\item 为模型提供清晰的进阶路径
			\item 允许为部分解决方案给予部分分数
			\item 模拟人类学习编码的方式
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{奖励函数的示例实现}
	\begin{block}{奖励函数逻辑 - 第1部分}
		\begin{enumerate}
			\item \textbf{格式检查:}
			\begin{itemize}
				\item 如果代码有正确的Triton语法 $\rightarrow$ 奖励 = 0.2
			\end{itemize}
			\item \textbf{编译检查:}
			\begin{itemize}
				\item 如果代码成功编译 $\rightarrow$ 奖励 = 0.5
				\item 如果编译失败 $\rightarrow$ 返回当前奖励
			\end{itemize}
		\end{enumerate}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{奖励函数的示例实现（继续）}
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\begin{block}{奖励函数逻辑 - 第2部分}
				\begin{enumerate}\setcounter{enumi}{2}
					\item \textbf{正确性检查:}
					\begin{itemize}
						\item 对每个测试用例，比较PyTorch和Triton结果
						\item 如果输出匹配 $\rightarrow$ 奖励 = 1.0
						\item 否则 $\rightarrow$ 奖励 = 0.6
					\end{itemize}
				\end{enumerate}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{关键要素}
				\begin{itemize}
					\item 多阶段评估过程
					\item 每个阶段建立在前一阶段之上
					\item 部分成功给予部分奖励
					\item 通过测试用例进行确定性验证
					\item 数值从0.0到1.0缩放
					\item 多个测试用例确保稳健性
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{训练循环和结果}
	\begin{block}{GRPO（基于梯度的奖励策略优化）如何工作:}
		\begin{itemize}
			\item \textbf{生成}: 使用采样为每个提示生成多个完成
			\item \textbf{评估}: 对每个完成运行奖励检查
			\item \textbf{更新}: 计算优势并反向传播信号
			\item \textbf{重复}: 模型精细策略以最大化奖励
		\end{itemize}
	\end{block}
	\begin{block}{结果:}
		\begin{itemize}
			\item 在5,000步后对保留示例的准确率为53\%
			\item 比OpenAI o1和DeepSeek-R1高出3倍的正确率
			\item 比Claude 3.7 Sonnet性能好4倍
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{\textbf{性能比较}: RFT与领先模型对比}
	\vspace{-0.2cm}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{center}
				\begin{tikzpicture}[scale=0.8, transform shape]
					\begin{axis}[
						ybar,
						axis lines=left,
						bar width=20pt,
						ylabel={\textbf{准确率 (\%)}},
						symbolic x coords={RFT, o1, DeepSeek-R1, Claude 3.7},
						xtick=data,
						xtick style={draw=none},
						ytick={0,10,20,30,40,50,60},
						ymin=0,
						ymax=60,
						nodes near coords={\textbf{\pgfmathprintnumber\pgfplotspointmeta}},
						width=6.5cm,
						height=5cm,
						major grid style={draw=gray!30},
						grid=major,
						]
						\addplot[fill=blue!80, draw=blue!90!black] coordinates {(RFT, 53) (o1, 18) (DeepSeek-R1, 17) (Claude 3.7, 13)};
					\end{axis}
				\end{tikzpicture}
			\end{center}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{\textbf{RFT训练进展}}
				\begin{itemize}\setlength{\itemsep}{0.4em}
					\item \textbf{起始点}: 5\%准确率
					\item \textbf{早期训练} (1,000步): 22\%
					\item \textbf{中期训练} (3,000步): 41\%
					\item \textbf{最终结果}: \textcolor{blue}{53\%准确率}
					\item 模型开发了超越有限训练示例的可泛化模式
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[shrink=15]
	\frametitle{与领先模型的性能比较}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{结果:}
				\begin{itemize}\setlength{\itemsep}{0.3em}
					\item 在5,000步后对保留示例的准确率为53\%
					\item 比OpenAI o1和DeepSeek-R1高出3倍的正确率
					\item 比Claude 3.7 Sonnet性能好4倍
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{Key Success Factors}
				\begin{itemize}\setlength{\itemsep}{0.3em}
					\item \textbf{奖励设计}: 多级奖励提供了清晰的学习信号
					\item \textbf{测试多样性}: 多样化的测试用例防止了过拟合
					\item \textbf{反奖励黑客}: 防止模型仅仅记忆输出
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\section{使用Unsloth实现实用的RFT}

\begin{frame}[shrink=60]
	\frametitle{Unsloth实用RFT工作流程}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{Unsloth是什么？}
				\begin{itemize}
					\item 用于高效LLM微调的开源库
					\item 可在有限硬件上运行（3GB+显存）
					\item 支持QLoRA、LoRA、RLHF、GRPO
					\item 比标准方法快2-4倍
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{开发工作流程}
				\begin{enumerate}
					\item \textbf{设置}: 安装依赖项
					\item \textbf{模型}: 选择基础模型
					\item \textbf{数据}: 格式化训练数据
					\item \textbf{配置}: 设置参数
					\item \textbf{训练}: 运行微调
					\item \textbf{部署}: 保存并服务
				\end{enumerate}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤1: 环境设置}
	\begin{block}{安装Unsloth}
		\begin{mintedbox}{bash}
# Basic installation
pip install unsloth

# For specific CUDA versions
# pip install unsloth[cu118]  # CUDA 11.8
# pip install unsloth[cu121]  # CUDA 12.1
		\end{mintedbox}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤1: 所需库}
	\begin{mintedbox}{python}
# 导入必要库
import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from transformers import TrainingArguments
	\end{mintedbox}
	\begin{block}{性能优化}
		\begin{itemize}
			\item 适用于显存有限设备的CPU卸载
			\item 用于更快训练的Flash Attention 2（在支持的GPU上）
			\item 梯度检查点以计算换内存
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤2: 选择模型与方法}
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\begin{block}{加载基础模型}
			\begin{mintedbox}{python}
max_seq_length = 2048
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3.1-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    load_in_4bit = True)
			\end{mintedbox}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{选择微调方法}
				\begin{itemize}
					\item \textbf{QLoRA}: 与LoRA结合的4位量化（最少资源）
					\item \textbf{LoRA}: 低秩适应（质量/资源平衡）
					\item \textbf{GRPO}: 用于DeepSeek风格的强化学习
					\item \textbf{全量微调}: 在高端硬件上获得最佳质量
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤2: 为RFT配置LoRA}
	\begin{block}{RFT特定配置}
		\begin{itemize}
			\item 针对键值注意力层以获得最高效的适应
			\item 对于强化学习等复杂任务使用秩16-32
			\item 添加dropout防止对奖励信号过拟合
			\item 启用梯度检查点以在训练期间节省内存
		\end{itemize}
	\end{block}
	\begin{block}{代码实现}
		\begin{mintedbox}{python}
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,  # LoRA rank
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"],
    alpha = 16,  # LoRA alpha
    dropout = 0.05,  # Add regularization
    use_gradient_checkpointing = True)
		\end{mintedbox}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤3: 数据集准备}
	\begin{block}{RFT的数据集格式}
		\begin{itemize}
			\item 格式化包含输入和预期输出的数据集
			\item 包含奖励信号或成功标准
			\item 确保不同场景下的多样化示例
		\end{itemize}
	\end{block}
	\begin{mintedbox}{python}
dataset = [
    {"input": "Solve: $2x + 3 = 7$", 
     "output": "To solve for x:\n$2x + 3 = 7$\n$2x = 4$\n$x = 2$"},
    {"input": "What is the capital of France?", 
     "output": "The capital of France is Paris."},
    # ...more examples
]
	\end{mintedbox}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤3: 数据集格式化为训练}
	\begin{block}{应用适当的模板}
		\begin{itemize}
			\item 使用模型的特定聊天模板格式
			\item 确保正确应用特殊标记
			\item 为您的用例设置适当的序列长度
		\end{itemize}
	\end{block}
	\begin{mintedbox}{python}
formatted_dataset = FastLanguageModel.format_dataset(
    dataset,
    tokenizer,
    max_seq_length,
    add_special_tokens = True,
    template = "<s>[INST] {input} [/INST] {output} </s>"
)
	\end{mintedbox}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤4: 配置训练参数}
	\begin{block}{训练参数}
	\begin{mintedbox}{python}
training_args = TrainingArguments(
    output_dir = "./results",
    num_train_epochs = 3,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 2,
    learning_rate = 2e-4,
    weight_decay = 0.01,
    max_grad_norm = 0.3,
    logging_steps = 10,
    save_total_limit = 3,
)
	\end{mintedbox}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤4: RFT的超参数选择}
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\begin{block}{关键超参数}
				\begin{itemize}
					\item \textbf{LoRA秩 (r)}: 在8-32之间；越高提供更强的表达能力
					\item \textbf{LoRA Alpha}: 通常与秩相同
					\item \textbf{学习率}: QLoRA为2e-4至5e-4
					\item \textbf{训练轮数}: 小数据集为2-5轮
					\item \textbf{Dropout}: 0.05-0.1用于正则化
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{RFT中避免过拟合}
				\begin{itemize}
					\item 如果出现过拟合迹象，增加dropout
					\item 通过监控损失实现早停
					\item 使用验证集评估泛化能力
					\item 添加权重衰减进行正则化
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤5: 执行训练}
	\begin{block}{执行训练}
		\begin{mintedbox}{python}
from trl import SFTTrainer

trainer = SFTTrainer(
    model = model,
    train_dataset = formatted_dataset,
    args = training_args,
    tokenizer = tokenizer,
    packing = False,
    dataset_text_field = "text",
)
		\end{mintedbox}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤5: 训练过程与监控}
	\begin{block}{运行训练}
		\begin{mintedbox}{python}
# Start training
trainer.train()
		\end{mintedbox}
	\end{block}
	\begin{block}{监控关键指标}
		\begin{itemize}
			\item \textbf{训练损失}: 应当稳定下降，但不应过快达到平台期
			\item \textbf{验证损失}: 监控过拟合迹象（验证损失上升）
			\item \textbf{学习率}: 记录学习率变化对性能的影响
			\item \textbf{GPU利用率}: 验证资源使用效率
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤6: 评估}
	\begin{block}{评估模型性能}
		\begin{itemize}
			\item 加载微调后的模型进行推理
			\item 创建结构化测试提示
			\item 在相同输入上与基线模型进行比较
			\item 测试多样化场景和边缘案例
			\item 测量推理速度和资源使用情况
		\end{itemize}
	\end{block}
	\begin{mintedbox}{python}
# Example test inference
prompt = "Solve: 3x + 5 = 20"
response = model.generate_text(prompt)
# Result: "To solve: 3x + 5 = 20
Subtract 5: 3x = 15
Divide by 3: x = 5"
	\end{mintedbox}
\end{frame}

\begin{frame}[fragile]
	\frametitle{步骤6: 部署}
	\begin{block}{保存和部署模型}
		\begin{itemize}
			\item 保存训练好的模型和分词器
			\item 优化推理性能（例如，ONNX转换） 
			\item 配置部署环境
			\item 设置监控和日志记录
		\end{itemize}
	\end{block}
	\begin{mintedbox}{python}
# Save the fine-tuned model
model.save_pretrained("./my-rft-model")
tokenizer.save_pretrained("./my-rft-model")
	\end{mintedbox}
\end{frame}

\begin{frame}
	\frametitle{步骤6: 持续改进}
	\begin{block}{持续的精细化}
		\begin{itemize}
			\item 在生产环境中收集用户反馈和模型输出
			\item 根据真实世界性能更新奖励函数
			\item 使用部署中发现的边缘案例扩展训练数据集
			\item 定期重新训练以纳入改进
		\end{itemize}
	\end{block}
	\begin{block}{长期维护}
		\begin{itemize}
			\item 监控随时间推移的性能退化
			\item 跟踪强化微调领域的新最佳实践 
			\item 评估更新到新的基础模型的收益
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{可直接使用的Unsloth笔记本}
	\begin{block}{可用的实现资源}
		\begin{itemize}
			\item Unsloth提供了适用于各种模型和任务的现成笔记本
			\item 可通过Google Colab或Kaggle访问（免费GPU资源）
			\item 包含监督微调和GRPO（强化微调）实现
		\end{itemize}
	\end{block}
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\begin{block}{流行模型}
				\begin{itemize}
					\item Llama 3.1 (8B)
					\item Phi-4 (14B)
					\item Mistral (7B, 22B)
					\item Qwen 2.5 (3B, 14B)
					\item Gemma 2 (2B, 9B)
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{专用变体}
				\begin{itemize}
					\item Qwen2.5-Coder (14B)
					\item CodeGemma (7B)
					\item Llama 3.2 Vision
					\item Qwen2-VL (7B)
					\item Phi-3 Medium
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
	\begin{center}
		\small{笔记本可在以下地址获取: \texttt{https://docs.unsloth.ai/get-started/unsloth-notebooks}}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{RFT的数据集构建}
	\begin{block}{入门指南}
		\begin{itemize}
			\item 确定数据集的目的: 聊天对话、结构化任务或领域特定数据
			\item 定义期望的输出风格: JSON、HTML、文本、代码或特定语言
			\item 确定数据来源: Hugging Face数据集、维基百科或合成数据
		\end{itemize}
	\end{block}
	\begin{columns}
		\begin{column}{0.48\textwidth}
			\begin{block}{常见数据格式}
				\begin{itemize}
					\item 纯文本格式
					\item 指令-输入-输出
					\item ShareGPT格式（多回合）
					\item ChatML（OpenAI风格）
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{数据集要求}
				\begin{itemize}
					\item 最少: 100个示例
					\item Optimal: 1,000+ examples
					\item Quality over quantity
					\item Can combine multiple datasets
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{RFT Implementation: Common Pitfalls (1)}
	\begin{itemize}
		\item \textbf{Problem}: Model produces generic or refuses to generate responses\\
		  \textbf{Solution}: Increase LoRA rank and alpha; ensure diverse training data
		\item \textbf{Problem}: Catastrophic forgetting (model loses pre-trained capabilities)\\
		  \textbf{Solution}: Lower learning rate; add regularization
		\item \textbf{Problem}: High training loss that doesn't converge\\
		  \textbf{Solution}: Check dataset formatting; reduce sequence length
	\end{itemize}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{RFT实施：常见问题 (2)}
	\begin{itemize}
		\item \textbf{问题}: 训练期间内存不足错误\\
		  \textbf{解决方案}: 启用梯度检查点；减小批量大小
		\item \textbf{问题}: 模型生成幻觉\\
		  \textbf{解决方案}: 实施惩罚虚构内容的奖励函数
		\item \textbf{问题}: 使用奖励信号时训练不稳定\\
		  \textbf{解决方案}: 归一化奖励；使用带有适当裁剪的PPO
	\end{itemize}
\end{frame}

\section{结论}

\begin{frame}[shrink=60]
	\frametitle{关键要点}
	\vspace{-0.2cm}
	\begin{columns}[T]
		\begin{column}{0.48\textwidth}
			\begin{block}{RFT的力量:}
				\begin{itemize}\setlength{\itemsep}{-0.1em}
					\item \textbf{自我提升}: 模型超越静态方法
					\item \textbf{数据效率}: 使用更少数据超越SFT
					\item \textbf{速度}: Turbo LoRA将处理量提高2-4倍
					\item \textbf{实际应用}: 超越学术领域
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.48\textwidth}
			\begin{block}{范式转变:}
				\begin{itemize}\setlength{\itemsep}{-0.1em}
					\item \textbf{传统方式}: 大数据 $\rightarrow$ 静态模型
					\item \textbf{RFT}: 最小数据 + 奖励 $\rightarrow$ 成长
					\item \textbf{新循环}: 持续改进
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{实施策略 - 核心组件}
	\begin{block}{所需核心组件}
		\begin{enumerate}
			\item 基础模型（开源LLM）
			\item 奖励函数定义
			\item 提示数据集（可以很小）
			\item 强化学习算法（RLHF、GRPO等）
			\item 评估框架
		\end{enumerate}
	\end{block}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{实施策略 - 最佳实践}
	\begin{block}{最佳实践}
		\begin{itemize}
			\item 从小型、清晰的奖励函数开始
			\item 构建全面的验证测试
			\item 实施反奖励黑客措施
			\item 监控演发行为
			\item 逐步增加复杂性
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[shrink=10]
	\frametitle{RFT的后续步骤}
	\begin{block}{开始使用:}
		\begin{itemize}
			\item 强化微调标志着LLM发展的重大飞跃
			\item 通过奖励信号而非标记示例进行训练
			\item 端到端平台使开发者能够轻松使用这种方法
			\item 专注于创新而非基础设施复杂性
		\end{itemize}
	\end{block}
	\begin{block}{RFT应用的成熟领域}
		\begin{itemize}
			\item \textbf{专业编程}: 领域特定的代码生成（嵌入式系统、高性能计算）
			\item \textbf{科学研究}: 提出并验证假设的模型
			\item \textbf{推理任务}: 复杂的逻辑和数学问题解决
			\item \textbf{教育}: 能够理解学生知识空白的自适应辅导系统
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}{}
	\begin{center}
		\LARGE Thank You!
	\end{center}
	\begin{center}
		\large Questions
	\end{center}
\end{frame}

\end{document}
